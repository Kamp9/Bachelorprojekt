\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{courier}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{microtype}
\usepackage{cleveref}

\renewcommand{\figurename}{Illustration}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=8.9in
\footskip=0.8in
\linespread{1.1}

\pagestyle{fancy}
\lhead{Troels Kamp Leskes}
\chead{Bachelorprojekt}
\rhead{18. Januar 2016}
\lfoot{\lastxmark}
\cfoot{}
\rfoot{Side\ \thepage\ af\ \protect\pageref{LastPage}}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}
\setlength\parindent{0pt}

\title{\vspace{25 mm}
\textbf{VEKTORISERING AF LINEÆR ALGEBRA} \\ \Large{Bachelorprojekt}\\
\ \newline
\large{Københavns Univesitet - Datalogisk Institut}}
\author{Troels Kamp Leskes \\ \texttt{kamp991@gmail.com} \\ BLR156}
\date{18. Januar 2016}

\begin{document}
\maketitle

\begin{tikzpicture}[remember picture,overlay]
  \node[inner sep=0pt] at (current page.center) {\includegraphics[page=1]{nat-farve.pdf}};
\end{tikzpicture}
\clearpage

\tableofcontents
\newpage

\section{Introduktion}

****** vs. lapack*******


\section{Generelt}
For at kunne løse problemer fra lineær algebra på en fornuftig måde, er det en nødvendighed at bruge forskellige algoritmer der manipulerer matricer. Der er mange forskellige algoritmer der kan være anvendelige når man ønske at regne med matricer. Eksempelvis kan lineære ligningssystemer: $$Ax=b$$
løses hvor $A$ er en kendt matrix, $b$ en kendt vektor og $x$ er en ukendt vektor.
En anden brugbar funktionalitet er at kunne finde en matrix invers eller at udregne egenværdierne***** for en given matrix. Det skal vise sig at mange af disse generelle problemer indenfor lineær algebra kan løses helt eller delvist ved brug af dekompositioner, der opdeler matricer således at problemer senere kan løses mere effektivt.

Problemet ved dekomposition, ligesom ved de fleste operationer på matricer er, at det er en tidskrævende proces. Alt efter algoritmen for dekompositionen kan køretiden variere men det er ikke billigt beregningsmæssigt. Det er derfor vigtigt, for at være så effektiv som muligt, at kende til hvornår forskellige algoritmer kan anvende med fordel frem for andre.\newline

Dette projekt omhandler hvorvidt data parallelisering kan opnås via vektorisering i forbindelse med matrixoperationer indenfor lineær algebra. Vektorisering gør parallelisering muligt og udregninger vil derfor kunne laves parallelt i stedet for sekventielt. Dette vil give muligheden for store beregningsmæssige forbedringer, hvilket i sidste ende betyder hurtigere beregning. Problemet ved vektorisering og dermed parallelisering er beregningsmæssige afhængigheder hvilket betyder at processer ofte bliver nødt til at vente på andre processer for at kunne starte. Dette skyldes at det data der skal benyttes endnu ikke er udregnet. Muligheder og begrænsninger omkring vektorisering vil derfor have stort fokus i dette projekt. 


\section{Gauss- og Gauss-Jordan Elimination}
Ved løsning af ligningssystemer er standardmetoden enten Gauss Elimination eller Gauss-Jordan Elimination.
Gauss Elimination består af to dele. Den første del, forward elimination, der ved brug af rækkeoperationer får matrixen på echelon form eller øvre triangulær form. Dernæst bruges anden del af algoritmen, back substitution, der ved at udregne en linje ad gangen nedefra udregner hele $x$ vektoren. Problemet med Gauss Elimination er at hvis vektoren $b$ ændres skal hele processen gøres forfra. Vi skal senere se at vi ved dekomposition af matrixen $A$ har mulighed for at undgå at gøre hele denne proces fra start, men i stedet bare ændre vektoren $b$. \newline

**** FIGUR AF GAUSS ELIMINATION *****\newline\newline
Gauss-Jorden Elimination virker på mange måder ligesom Gauss Elimination men i stedet for at stoppe algoritmen efter matrixen er på echelonform fortsættes forward eliminationen indtil at matrixen er på reduceret echelonform.\newline\newline
**** FIGUR AF GAUSS-JORDAN ELIMINATION *****
\newline\newline

Køretiden for både Gauss- og Gauss-Jordan Elimination er $\mathcal{O}(n^3)$ uafhængigt af om matrixen $A$ ikke ændres. Vi skal se i næste afsnit at man ved brug dekomposition kan løse ligningssystemet for forskellige vektorer $b$ og en uændret matrix $A$ i $\mathcal{O}(n^2)$ når først dekompositionen for $A$ er udregnet i køretiden $\mathcal{O}(n^3)$.

\section{Dekomposition}
Der er flere forskellige typer af dekompositioner. I dette projekt vil vi se nærmere på Cholesky- og LU dekomposition. Hvilken dekomposition man ønsker at benytte har ofte noget at gøre med hvilke egenskaber den givne matrix har. Hvis samtlige egenværdier i matrixen er positive siges at matrixen er positiv definit. At samtlige egenværdier er positive vil også sige at matrixen er symmetrisk. Hvis en matrix er positivt definit er det muligt at anvende Cholesky dekomposition. Hvis matrixen ikke er positivt definit er det nødvendigt at benytte en dekomposition der foretager mere arbejde end Cholesky dekompositionen, et eksempel på sådan en dekomposition er LU dekomposition.

\subsection{Cholesky dekomposition}
En simpel form for dekomposition er Cholesky dekomposition.
Cholesky bruges til at opdele en symmetrisk positiv definit matrix $A$ således:
$$A = LL^*$$
Hvor $L$ en nedre triangulær matrix og $L^*$ er den konjugerede transponerede.\vspace{\baselineskip}
Generelt for reelle tal vil Cholesky dekomposition have formen:
$$A = LL^T$$
Hvor $L^T$ er $L$ transponeret.\vspace{\baselineskip}
Hvorvidt $A$ er positiv definit kan bestemmes ud fra egenværdierne for matrixen $A$ der alle er positive for en positiv definit matrix. Man kan dermed undersøge hvorvidt Cholesky dekomposition er anvendeligt på den pågældende matrix. Hvis det ikke er muligt at benytte Cholesky algoritmen er man nødsaget til eksempelvis at bruge den mere tidskrævende LU dekomposition.
Der er to primære fordele ved Cholesky dekomposition, den ene er at man kun behøver at beregne enten den nedre- eller øvre triangulær matrix og dermed kan man hurtigt den modsatte halvdel ved transponering, dette giver det halve arbejde i forhold til eksempelvis LU dekompomposition. Den anden fordel ved Cholesky dekomposition er at det ikke nødvendigt at andvende pivotstrategier i matrixen når man har en positiv definit matrix. Andvendelse af pivotelementer er en nødvendighed for at undgå numerisk ustabilitet, vi vil komme ind på i forbindelse med LU dekomposition. \newline

For at beskrive Cholesky dekomposition har vi følgende rekursive formler for udregningen af den nedre triangulær matrix $L$: 
\begin{align*}
u_{ii} &= \sqrt{a_{ii} - \sum\limits_{k=1}^{i-1} u_{ki}'}\\
u_{ij} &= \frac{a_{ij} - \mathlarger{\sum\limits_{k=1}^{i-1}} u_{ki}u_{kj}}{u_{ii}} , \ \ \ \text{for } j=i + 1, ..., n
\end{align*}

****
Forklaring af rekursive formler
****

\subsection{LU dekomposition}
I LU dekomposition opdeles en matrix $A$ i en nedre- og øvre triangulær matrix $L$ og $U$ således:
$$A = LU$$
LU dekomposition har den fordel at kunne bruges på matricer der ikke er positivt definit og dermed kan LU dekomposition bruges hvor Cholesky algoritmen ikke kan anvendes. Selvom LU dekomposition dermed virker på væsentlig flere matricer end Cholesky dekomposition er der stadig mulighed for at LU dekomposition ikke kan benyttes da numerisk ustabilitet samt division med $0$ kan forekomme. Numerisk ustabilitet opstå ved division med reelle tal der er tæt på $0$ og afrundingsfejl vil derfor have mulighed for at nedarve gennem algoritmen og man vil få et mere upræcist resultat end forventet. LU dekomposition virker i modsætning af Cholesky, følgende af symmetrien, også på ikke kvadratiske matricer. Dog vil vi i dette projekt have vores fokus på kvadratiske matricer.
\newline
**** 
Ved pivot, er antallet af tilfælde hvor LU dekomposition opfører kun forekommende hvis matrixen har rang mindre end $n$ hvilket vil sige at der ikke er én løsning for ligningssystemet.
****
\newline

\section{Pivot}
Et generelt problem der opstår ved både dekompositioner og Gauss Elimination, er at de kan være numerisk ustabile, hvilket kan føre til at fejl nedarver sig igennem algoritmen og bliver større og større på grund af afrundingsfejl.
I værste fald kan manglen på pivot anvendelse betyde at algoritmen fejler fordi man forsøger at dividere med $0$.
\newline\newline
****eksempel på division med $0$****
\newline\newline
For at undgå dette samt numerisk ustabilitet kan det derfor være nødvendigt at indføre pivot. Metoden for pivot er, at inden hver iteration i algoritmen, til eksempelvis LU dekomposition eller Gauss Elimination, at sørger for at permutere matrixen $A$ således at det største mulige absolutte element i $A$ er i det øverste venstre hjørne af det udsnit af matrixen der arbejdes på.
\newline
Pivotmetoden er i hver iteration afhængig af den nyligt udregnede matrix $A'$. Da man efter hver operation muligvis ændre på hvilket element der næste gang vil have størst absolut værdi. Det er derfor en nødvendighed for hver iteration i algoritmen at undersøge hvilke permutationer der skal foretages før at numerisk stabilitet kan understøttes. Det medfølger derfor at det ikke er muligt at foretage samtlige permutationer på en gang inden algoritmen foretages. Det er derfor altid nødvendigt at finde pivotelemnter imens en given algoritme foretage, hvilket gør udregningen væsentlig mere kompliceret samtidig med at paralleliteten besværliggøres.
I dette projekt vil vi komme ind på tre pivotstrategier: Partial-, Complete- og Rook Pivot. Hver har de fordele og ulemper med hensyn til hastighed og numerisk stabilitet, hvilket vil blive præsenteret i deres repræsentative afsnit.
\newline\newline
For at holde styr på hvilke permutationer der er foretaget er det nødvendigt at indføre permutationsmatricer hvilket gør det muligt for os at komme tilbage til den oprindelige matrix $A$. Partial Pivot strategien der kun ombytter rækker i matrixen vil kun skulle bruge en enkelt permutrationsmatrix $P$. Samtidig vil Complete og Rook strategierne der begge ombytter både rækker og kolonner have to permutations matricer nemlig $P$ der holder styr på rækkeombytninger og $Q$ der noterer ombytning af kolonner.
\newline\newline
***Lapack biblioteket benytter sig af Partial Pivot og dermed vinder de en del hastighed i bytte for numerisk stabilitet. Dette betyder endvidere at det er muligt at lave algoritmer der giver mere præcise resultater end Lapack biblioteket. Hvilket pivotstrategi man vælger er altså et spørgsmål om man ønske de mest præcise resultater eller øget hastighed.*****
\newline\newline
****
Tænk over hvad der sker med rangen for partiel- og Rook Pivot. LU virker ok når rangen er $<$ end $n$, men stopper før tid.
****
\newline

\subsection{Partial Pivot}
Partial pivot (PP) er den hurtigste af de tre metoder hvor der i den $k$'te iteration undersøges $A_{k:n, k}$ for den største absolutte værdi. Dette løber $n - k$ elementer igennem for hver iteration i en $n \times n$ matrix hvor $k = 0, ..., n-1$ . Målet er at placere den største indgang for hver iteration i det udsnit for matrixen vi arbejder med øverst. Dette kræver $n-(k+1)$ sammenligninger, hvilket giver os en køretid for hver iteration på $\mathcal{O}(n-k)$ så for en $n \times n$ matrix har vi antallet af operationer for at søge efter pivotelementer til at være:
$$
\sum\limits_{k=0}^{n-1} \left(n-k\right) = \sum\limits_{k=1}^{n} k = \frac{n(n+1)}{2} = \frac{n^2}{2} + \frac{n}{2}
$$
Hvilket svarer til $\mathcal{O}(\frac{n^2}{2})$ \newline\newline
Da PP kun undersøger én søjle for hver iteration i $A$ har man et problem hvis den søjle hvor pivotelement hentes har meget mindre elementer end elementerne i resten af den matrix. Denne forskel på størrelsen af elementer mellem pivotelemntet og resten af elementerne i matrixen giver numerisk ustabilitet og dermed har vi muligheden for at få et resultat der afviger meget i forhold til hvad algoritmen faktisk burde returnere. PP er derfor god fordi metoden er hurtig, men samtidig også den mest numereisk ustabil af de tre pivotstrategier.
\newline\newline
****Metoden for PP kan ses på illustration \ref{fig:partial} ****
\newline\newline

\begin{figure}
\resizebox{\textwidth}{!}{
$
 \begin{bmatrix}
  \boldsymbol{x_{11}} & x_{12} & x_{13} & x_{14} \\
  \boldsymbol{x_{21}} & x_{22} & \gamma_{23} & \gamma_{24} \\
  \boldsymbol{\gamma_{31}} & x_{32} & x_{33} & x_{34} \\
  \boldsymbol{x_{41}} & \gamma_{42} & x_{43} & x_{44}
 \end{bmatrix}
\xrightarrow[r_1, r_3]{\text{swap}}
 \begin{bmatrix}
  U_{31} & U_{32} & U_{33} & U_{34} \\
  L_{21} & \boldsymbol{x_{r2}} & x_{r2} & \gamma_{r2} \\
  L_{11} & \boldsymbol{x_{r1}} & \gamma_{r1} & x_{r1} \\
  L_{41} & \boldsymbol{\gamma_{r4}} & x_{r4} & x_{r4}  
 \end{bmatrix}
\xrightarrow[r_2, r_4]{\text{swap}}
\begin{bmatrix}
  \gamma_{r3} & x_{r3} & x_{r3} & x_{r3} \\
  x_{r4} & \gamma_{r4} & x_{r4} & x_{r4} \\
  x_{r1} & x_{r1} & \boldsymbol{\gamma_{r1}} & x_{r1} \\
  x_{r2} & \gamma_{r2} & \boldsymbol{x_{r2}} & x_{r2}  
 \end{bmatrix}
\xrightarrow[r_2, r_4]{\text{swap}}
\begin{bmatrix}
  \gamma_{r3} & x_{r3} & x_{r3} & x_{r3} \\
  x_{r4} & \gamma_{r4} & x_{r4} & x_{r4} \\
  x_{r1} & x_{r1} & \boldsymbol{\gamma_{r1}} & x_{r1} \\
  x_{r2} & \gamma_{r2} & \boldsymbol{x_{r2}} & x_{r2}  
 \end{bmatrix}
$
}
\caption{*****PP for en vilkårlig $4 \times 4$ matrix******}
\label{fig:partial}
\end{figure}


\subsection{Complete Pivot}
I modsætning til PP finder Complete Pivot (CP) den største absolutte værdi i hele den pågældende delmatrixe, i stedet for kun at se på søjlen i iterationen. CP sammenligner derfor følgende elementer $A_{k:n, k:n}$ hvilket for hver iteration giver $(n - k)^2$ elementer der skal sammenlignes. Heraf får vi altså antallet af sammenligninger til at være:
\[
\sum\limits_{k=0}^{n-1} \left(n-k\right)^2 = \sum\limits_{k=1}^{n} k^2 = \frac{n}{6}(n+1)(2n+1) = \frac{n^3}{3}+\frac{n^2}{2}+\frac{n}{6}
\]
Hvilket svarer til $\mathcal{O}(\frac{n^3}{3})$ \vspace{\baselineskip}

CP giver den højeste numerisk stabilitet af de tre pivotstrategier men er samtidig den mest tidskrævende metode at udføre. Dette følger klart af at antallet af elementer der skal sammenlignes er størst hvis hele delmatrixen gennemsøges for den største absolutte værdi. Hvis man derfor ønsker meget præcise resultater vil CP være et godt valg da man imodsætning til PP ikke behøver at frygte at algoritmen giver upræcise resultater som følge af at matrixens elementer ser ud på en bestemt måde. Rook Pivot som vi skal se på i næste del omhandler hvordan høj numerisk stabilitet kan opnås uden man er nødsaget til at søge samtlige elementer i gennem for hver iteration i algoritmen.
\newline\newline
****Metoden for CP kan ses på illustration \ref{fig:complete} ****
\newline\newline

\begin{figure}
\resizebox{\textwidth}{!}{
$
 \begin{bmatrix}
  \boldsymbol{x_{11}} & \boldsymbol{x_{12}} & \boldsymbol{x_{13}} & \boldsymbol{x_{14}} \\
  \boldsymbol{x_{21}} & \boldsymbol{x_{22}} & \boldsymbol{x_{23}} & \boldsymbol{\gamma_{24}} \\
  \boldsymbol{x_{31}} & \boldsymbol{x_{32}} & \boldsymbol{x_{33}} & \boldsymbol{x_{34}} \\
  \boldsymbol{x_{41}} & \boldsymbol{x_{42}} & \boldsymbol{x_{43}} & \boldsymbol{x_{44}}  
 \end{bmatrix}
\xrightarrow[\overset{r_1, r_2}{c_1, c_4}]{\text{swap}}
 \begin{bmatrix}
  U_{24} & U_{22} & U_{23} & U_{21} \\
  L_{14} & \boldsymbol{x_{r2}} & \boldsymbol{x_{r2}} & \boldsymbol{x_{r2}} \\
  L_{11} & \boldsymbol{x_{r1}} & \boldsymbol{x_{r1}} & \boldsymbol{x_{r1}} \\
  L_{44} & \boldsymbol{\gamma_{r4}} & \boldsymbol{x_{r4}} & \boldsymbol{x_{r4}}  
 \end{bmatrix}
\xrightarrow[\overset{r_3, r_4}{c_3, c_4}]{\text{swap}}
\begin{bmatrix}
  \gamma_{r3} & x_{r3} & x_{r3} & x_{r3} \\
  x_{r4} & \gamma_{r4} & x_{r4} & x_{r4} \\
  x_{r1} & x_{r1} & \boldsymbol{\gamma_{r1}} & x_{r1} \\
  x_{r2} & \gamma_{r2} & \boldsymbol{x_{r2}} & x_{r2}
 \end{bmatrix}
\xrightarrow[\overset{r_3, r_4}{c_3, c_4}]{\text{swap}}
\begin{bmatrix}
  \gamma_{r3} & x_{r3} & x_{r3} & x_{r3} \\
  x_{r4} & \gamma_{r4} & x_{r4} & x_{r4} \\
  x_{r1} & x_{r1} & \boldsymbol{\gamma_{r1}} & x_{r1} \\
  x_{r2} & \gamma_{r2} & \boldsymbol{x_{r2}} & x_{r2}  
 \end{bmatrix}
$
}
\caption{****CP for en vilkårlig $4 \times 4$ matrix*****}
\label{fig:complete}
\end{figure}

\subsection{Rook Pivot}

Rook Pivot (RP) er den sidste af de tre strategier og samtidig den der skiller sig mest ud fra de andre. Da vi i stedet for at undersøge et på forhånd givet antal elementer i hver iteration, leder efter elementer i matrixen der har den højeste absolutte værdi både i sin række og kolonne. Dette medfører at vi på forhånd ikke kender antallet af elementer der skal undersøges før det ønskede pivotelement er lokaliseret. Eftersom at vi først undersøger den første kolonne**** i den givne iteration, for derefter at undersøge om dette element samtidig er det største i rækken, kan vi altså være heldige at algoritmen stopper allerede efter $2(n-k)$, hvilket svare til to gange PP sammenligninger. $2(n-k)$ er dermed den nedre grænse for RP. På den anden side har man også muligheden for at man hver gang finder et nyt element der er større end det forrige man undersøgte. Dette kan dog maximalt medføre at man undersøger hele delmatrixen i den pågældende iteration, og vi kommer frem til en øvre grænse på $(n-k)^2$, hvilket svarer til antallet af sammenligninger i CP. Vi har altså en kørertid for RP mellem $\Omega(2(n-k))$ og $\mathcal{O}((n-k)^2)$. Det er dermed ikke til at vide præcist hvor mange elementer man skal undersøge ved at benytte RP. \newline\newline
RP forsøger derfor at være det bedste fra to verdener. På den ene side får vi muligheden for en hastighed der minder meget om PP samtidig med vi aldrig overskrider antallet af sammenligninger fra CP. På den anden har vi samtidig i numerisk stabilitet der minder om CP uden at foretage den samme mængde beregninger.
\newline\newline
***** Det forventede antal sammenligninger for RP er $\mathcal{O}(\frac{3n^2}{2})$ ifølge \cite{Rook} Corollary 3.11:

\begin{quote}
\textit{"The expected total number of compares $E_T$ required to locate all $(n-1)$ Rook's pivots is\linebreak
$\mathcal{O}(\frac{3n^2}{2}) = \mathcal{O}(n^2)$"}
\end{quote}
Hvilket betyder at antallet af teoretiske forventede operationer, kun er tre gange flere end PP. Den store fordel ved RP er at man ved emipirisk analyse kan vise at den nummeriske stabillitet for RP er meget tæt på CP \cite{Rook}, samtidig med at man spare mange af sammenligningerne nødvendigt for at lave CP. Metoden hvorpå man i \cite{Rook} afgøres hvorvidt en strategi er numerisk stabil 


*****teoretisk vs. empirisk*****
****Metoden for RP kan ses på illustration \ref{fig:Rook} ****
\newline\newline

\begin{figure}
\resizebox{\textwidth}{!}{
$
 \begin{bmatrix}
  \boldsymbol{x_{11}} & \boldsymbol{x_{12}} & \boldsymbol{x_{13}} & \boldsymbol{x_{14}} \\
  \boldsymbol{x_{21}} & \boldsymbol{x_{22}} & \boldsymbol{x_{23}} & \boldsymbol{\gamma_{24}} \\
  \boldsymbol{x_{31}} & \boldsymbol{x_{32}} & \boldsymbol{x_{33}} & \boldsymbol{x_{34}} \\
  \boldsymbol{x_{41}} & \boldsymbol{x_{42}} & \boldsymbol{x_{43}} & \boldsymbol{x_{44}}  
 \end{bmatrix}
\xrightarrow[\overset{r_1, r_2}{c_1, c_4}]{\text{swap}}
 \begin{bmatrix}
  U_{24} & U_{22} & U_{23} & U_{21} \\
  L_{14} & \boldsymbol{x_{r2}} & \boldsymbol{x_{r2}} & \boldsymbol{x_{r2}} \\
  L_{11} & \boldsymbol{x_{r1}} & \boldsymbol{x_{r1}} & \boldsymbol{x_{r1}} \\
  L_{44} & \boldsymbol{\gamma_{r4}} & \boldsymbol{x_{r4}} & \boldsymbol{x_{r4}}  
 \end{bmatrix}
\xrightarrow[\overset{r_3, r_4}{c_3, c_4}]{\text{swap}}
\begin{bmatrix}
  \gamma_{r3} & x_{r3} & x_{r3} & x_{r3} \\
  x_{r4} & \gamma_{r4} & x_{r4} & x_{r4} \\
  x_{r1} & x_{r1} & \boldsymbol{\gamma_{r1}} & x_{r1} \\
  x_{r2} & \gamma_{r2} & \boldsymbol{x_{r2}} & x_{r2}
 \end{bmatrix}
\xrightarrow[\overset{r_3, r_4}{c_3, c_4}]{\text{swap}}
\begin{bmatrix}
  \gamma_{r3} & x_{r3} & x_{r3} & x_{r3} \\
  x_{r4} & \gamma_{r4} & x_{r4} & x_{r4} \\
  x_{r1} & x_{r1} & \boldsymbol{\gamma_{r1}} & x_{r1} \\
  x_{r2} & \gamma_{r2} & \boldsymbol{x_{r2}} & x_{r2}  
 \end{bmatrix}
$
}
\caption{****RP for en vilkårlig $4 \times 4$ matrix*****}
\label{fig:Rook}
\end{figure}



\subsection{Diskussion af hvilken teknik der er bedst}


\section{Motivation}

******
\begin{align*}
\intertext{Antallet af operationer for selve LU dekomposition:}
T_{lud} &= \left(\frac{8n^3}{3} + 4n^2 - \frac{20n}{3}\right)\\
\intertext{Antallet af operationer for forward substitution:}
T_{fs} &= (4n^2-4n)\\
\intertext{Antallet af operationer for back substitution:}
T_{bs} &= (4n^2 + 12n)\\
\end{align*}

Adderes antallet af operationer for LU dekompositionen, forward- og backward substitution får vi løsningen af ligningssystemet til at tage:

\begin{align*}
T_{lu} &= \left(\frac{8n^3}{3} + 4n^2 - \frac{20n}{3}\right) + T(4n^2-4n)+T(4n^2+12n)\\
T_{lu} &= \left(\frac{8n^3}{3} + 12n^2 + \frac{4n}{3}\right)
\end{align*}

Grunden til at antallet af operationer for forward substitution er mindre end back substitution er at den nedre triangulære matrix $L$ har ettaller i diagonalen mens værdierne i den øvre triangulære matrix $U$ har en diagonal der kan variere. Derfor er det muligt at undgå udregninger per iteration i $L$, der bliver brugt i forward substitution, hvilket gør at undregningstiden falder i forhold til back substitution.\vspace{\baselineskip}

Gauss Elimination har operationerne opdelt i to operationer, forward elimination delen og back substitution som vi også så under LU dekompositionen. 

\begin{align*}
\intertext{Antallet af operationer for forward elimination der får matrixen på echelon- eller øvre triangulær form:}
T_{fe} &= \left(\frac{8n^3}{3} + 8n^2 - \frac{32n}{3}\right)\\
\intertext{Antallet af operationer for back substitution er den samme som set i LU dekomposition altså:}
T_{bs} &= (4n^2+12n)\\
\intertext{Deraf gives antallet af operation for løsning af ligningssystemer med Gauss Elimination til at være:}
T_{ge} &= \left(\frac{8n^3}{3} + 8n^2 - \frac{32n}{3}\right) + T(4n^2 + 12n)\\
T_{ge} &= \left(\frac{8n^3}{3} + 12n^2 + \frac{4n}{3}\right)
\end{align*}

Hvilket er det samme som antallet af operationer ved brug af LU dekomposition, umiddelbart er der altså ikke noget at vinde hvis man kun bruger LU dekompositionen én gang, da det er lige så tidskrævende at opdele matrixen i $L$ og $U$ for derefter at løse for $b$ som det er at udregne disse skridt samtidigt som er tilfældet i Gauss Elimination.  \vspace{\baselineskip}

Fordele ved LU dekomposition kommer eksempelvis hvis man har mulighed for at genbruge dekompositionen. Det simpleste tilfælde er at man har en matrix $A$ der ikke ændres og ønske at finde flere løsninger for forskellige søjlevektorer $b$. Da vil man i LU dekomposition kunne nøjes med at udregne dekompositions delen en enkelt gang og skifte værdier for forward- og back substitutionen. Modsat vil Gauss Elimination skulle udregne både forward eliminationen og back substitutionen for hver ny søjlevektor $b$. Det samme gør sig gældende hvis man skal finde en matrixs inverse.\vspace{\baselineskip}

\section{forward- og back substitution}
Forward- og backsubstitution er begge metoder til at løse ligningssystemer ud fra en given triangulær matrix $L$ eller $U$ samt en søjlevektor $b$. Forward substitution kan benyttes ved nedre triangulære matricer og udnytter substitution for den tidligere iteration. Den første værdi er mulig at finde da der kun er én ubekendt og resten tager de tidligere beregnede værdier med videre til eliminering af ubekendte. Algoritmen kan beskrives således:

\begin{align*}
x_1 &= b_1/a_{11}\\
x_2 &= (b_2 - a_{21} x_1)/a_{22}\\
x_3 &= (b_3 - a_{31}x_1-a_{32}x_2)/a_{33}\\
\vdots\\
x_m &= (b_m -a_{m1}x_1-a_{m2}x_2- ... -a_{m,m-1}x_{m-1})/a_{mm}
\end{align*}

På samme måde benyttes back substitution på en øvre triangulær matrix, at regne nedefra og op og eliminere ubekendte ud fra tidligere udregninger. Algoritmen kan på samme måde beskrives således:

\begin{align*}
x_m &= b_m/a_{mm}\\
x_{m-1} &= (b_{m-1} - a_{m-1,m}x_m )/a_{m-1,m-1}\\
x_{m-2} &= (b_{m-2} - a_{m-2,m-1}x_{m-1} - a_{m-2, m} x_m )/a_{m-2,m-2}\\
\vdots\\
x_1 &= (b_{1} - a_{12}x_{2} - a_{13} x_3 - ... - a_{1m}x_m )/a_{11}
\end{align*}

****
For at løse ligningssystemer ved LU dekomposition er benyttes både forward- og back substitution. Vi kan som før vist skrive dekomponere matrixen $A$ til $LU$. Derefter er det muligt at udregne $(Ux) = z$ ved 

$$Ax=(LU)x=L(Ux)=Lz=b$$
Først findes $z$ så $Lz=b$. Dette gøres med forward substitution\newline
Dernæst findes $x$ så $Ux=z$. Dette gøres med back substitution
****
\newline\newline
*****
bliv enig om vi går til m eller n
*****
\newline
\begin{align*}
z_1	&= \frac{b_1}{l_{11}}\\
z_i	&= \frac{1}{l_{ii}} \left(b_i - \sum\limits_{j=1}^{i-1} l_{ij} z_j\right), \ \ \ \text{for } i=2, ..., n
\end{align*}

\begin{align*}
x_n	&= \frac{z_n}{u_{nn}}\\
x_i	&= \frac{1}{u_{ii}} \left(z_i - \sum\limits_{j=i+1}^{n} u_{ij} x_j\right), \ \ \ \text{for } i=n-1, ..., 1
\end{align*}




\newpage
\section{Plan for projekt}
\begin{verbatim}
Introduktion

- Baggrundsstof
- Definitioner
- Motivation (hvorfor er LU et godt sted at starte)


Problemorienteret analyse
(Skal beskrive problemet, så en anden med samme forudsætninger
kan skrive programmerings overvejelser)

- LU metoder
  - Complete
  - Partial
  - Rook
  
-Algoritmer ligesom forward- og back substitution

-Intuitiv forklaring af algoritmerne, evt udledende

-Figurer som tavlebilleder

-Antal af operationerne, enten eksakt eller store O
  
  => MAV: Motivation, alternativer, valg
     Fordele, ulemper, hvad vælger jeg

- Implementationer
  - SciPy, hvad gør de?
    LaPACK, ulemper, kræver aligned memory, BLAS

  - Håndkodet sekventielt (C - kode, evt. python)
    Hvordan ville man gøre det, hvad forventes af implementation
    (hastighed?), ulemper (arkitektur specifikt, svært at
    vedligeholde)

  - Vektorisering
    Hvorfor er det ikke den gyldne løsning?
    Afhængigheder
    eks. Cholesky

  => MAV
    Rook som vektoriseret

- Vektorbaseret programmering
  - biblioteker
  - sprog
  => MAV
    Hvorfor NumPy

Programmerings overvejelser
(Skal beskrive problemet uden at tage valg mht. til f.eks. sprog,
således at en anden kan implementere en lignende løsning)

- Begrænsninger i vektor udtryk
- Strategi, asymptotisk køretid
- Lagerforbrug (hukommelse)
- Arkitektur, prefetch, cachemodeller,

Vektoriserings strategi
  -Partial
  -Complete
  -Rook

Programbeskrivelse
(Skal beskrive særlige dele af den konkrete implementation)

- Swap


Afprøvning
- Korrekthed
  - Sammenligning med SciPy
  - Numerisk stabilitet
  - Randtilstande (edge conditions)
- Performance
  - Sammenligning med SciPy
  - Andre?
  - Hastighed, hukommelsesforbrug
\end{verbatim}

\pagebreak
\section{Forbudt tekst fra motivation}
En anden operation hvor LU dekomposition skal vise sig at være mere effektiv end Gauss Elimination er for at finde en matrix' inverse $A^{-1}$: 
$$
AA^{-1} = I
$$
Hvor $I$ er identitesmatrixen med samme dimension som $A$.\vspace{\baselineskip}

Fordelen ved LU dekompositionen i dette tilfælde er at man har mulighed for at bevare en del af sin udregning gennem algoritmen. I modsætning til Gauss Elimination der ikke kan genbruge noget og dermed bliver nødt til at foretage flere beregninger.\vspace{\baselineskip}

For at udregne den inverse gennem LU dekomposition er det derfor kun nødvendigt at udregne dekompositionen én gang hvorefter forward- og back substitution hver skal beregnes $n$ gange. Derfor får vi antallet af operationer til at være:

\begin{align*}
&= 1 \times T\left(\frac{8n^3}{3} + 4n^2 - \frac{20n}{3}\right) + n \times T(4n^2 - 4n) + n \times T(4n^2 + 12n)\\
&= T\left(\frac{8n^3}{3} + 4n^2 - \frac{20n}{3}\right) + T(4n^3 - 4n^2) + T(4n^3 + 12n^2)\\
&= T\left(\frac{32n^3}{3} + 12n^2 - \frac{20n}{3}\right)
\end{align*}

Dermed er den asymptotiske køretid for udregning af matrix inverse ved LU dekomposition $\mathcal{O}(n^3)$\vspace{\baselineskip}

Gauss Elimination på den anden side vil være nødsaget til at udregne både forward elimination samt back substitution for hvert $n$ og antallet af operationer vil derfor være:

\begin{align*}
&= n \times T\left(\frac{8n^3}{3} + 8n^2 - \frac{32n}{3}\right) + n \times T(4n^2 + 12n)\\
&= T\left(\frac{8n^4}{3} + 8n^3 - \frac{32n^2}{3}\right) + T(4n^3 + 12n^2)\\
&= T\left(\frac{8n^4}{3} + 12n^3 - \frac{4n^2}{3}\right)
\end{align*}

Den asymptotiske køretid for udregning af matrix inverse ved Gauss Elimination er dermed $\mathcal{O}(n^4)$.\vspace{\baselineskip}
\newline\newline
***** DERFOR ER LU GODT måske sammenlign køretider for inverse og fixed matrix.
*****
\newline\newline

The time is calculated by first separately calculating the number of additions, subtractions,
multiplications, and divisions in a procedure such as back substitution, etc. We then assume
4 clock cycles each for an add, subtract, or multiply operation, and 16 clock cycles for a
divide operation as is the case for a typical AMD®-K7 chip
\url{http://mathforcollege.com/nm/mws/gen/04sle/mws_gen_sle_txt_ludecomp.pdf}
***








\pagebreak
\begin{thebibliography}{9}

\bibitem{0} LU, QR and Cholesky Factorizations using Vector Capabilities of GPUs

\bibitem{1} Vectorized LU Decomposition Algorithms for Large-Scale Circuit Simulation

\bibitem{2}LU-GPU: Efficient Algorithms for Solving Dense Linear Systems on Graphics Hardware

\bibitem{Rook}The Rook’s pivoting strategy

\bibitem{Old_Rook}L. Neal, G. Poole, A Geometric analysis of Gaussian elimination, II, Linear Algebra Appl. 173 (1992) 239–264.

\bibitem{4} \url{https://www.cs.umd.edu/sites/default/files/scholarly_papers/jjung_1.pdf}

\bibitem{5} \url{https://courses.engr.illinois.edu/cs554/fa2013/notes/07_cholesky.pdf}

\bibitem{6} \url{https://courses.engr.illinois.edu/cs554/fa2013/notes/06_lu_8up.pdf}

\bibitem{7} \url{https://www.cs.utexas.edu/~plapack/icpp98/node2.html}

\bibitem{8} \url{http://www.staff.science.uu.nl/~bisse101/Book/PSC/psc2_3.pdf}

\bibitem{9} \url{http://www.seas.ucla.edu/~vandenbe/103/lectures/lu.pdf}

\end{thebibliography}

\end{document}

